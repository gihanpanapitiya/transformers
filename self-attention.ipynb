{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f547eb9e-2baf-4d8a-ac89-ef3c2228d0b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10c8bac70>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import pickle\n",
    "import pandas as pd \n",
    "torch.manual_seed(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05c9f20",
   "metadata": {},
   "source": [
    "let's define the main parameters of the sel attention mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e26e4bca-5566-4436-b24b-cb530c2f0a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding dimension; how many features we use for each token\n",
    "embed_dim = 4\n",
    "\n",
    "# number of attention heads\n",
    "num_heads = 2\n",
    "\n",
    "# how many tokens we have\n",
    "seq_length = 4\n",
    "\n",
    "# how many batches\n",
    "batch_size = 2\n",
    "\n",
    "# how many features per head\n",
    "head_dim = embed_dim // num_heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0b97074-f184-4371-95b6-c82bfeedf60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the embeddings. this will determine a feature vector for each distint token\n",
    "embed = torch.nn.Embedding(5, embed_dim, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7fac65bc-f794-4779-857a-5ebbb475d1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the input features\n",
    "# We have two data points. The first one has features 1 and 2, the other two positions contain no features.\n",
    "# The second data point has 4 characters\n",
    "                  # A B,_,_    A,B,C,D\n",
    "s =  torch.tensor([[1,2,0,0], [1,2,3,4] ]) # here 0 is the padding token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0b7ee31-387f-47b8-b9e7-f42765bac882",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 0, 0],\n",
       "        [1, 2, 3, 4]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4214645-59d0-455f-9048-7e409a83b9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embed the input features\n",
    "e = embed(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42caf2cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0299, -0.0498,  1.0651,  0.8860],\n",
       "         [-0.8110,  0.6737, -1.1233, -0.0919],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[ 0.0299, -0.0498,  1.0651,  0.8860],\n",
       "         [-0.8110,  0.6737, -1.1233, -0.0919],\n",
       "         [ 0.1405,  1.1191,  0.3152,  1.7528],\n",
       "         [-0.7396, -1.2425, -0.1752,  0.6990]]], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52bcd663-15b3-4cf6-a8f6-07ab495e6475",
   "metadata": {},
   "source": [
    "Now, each characrer has been represented using a 4-element vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f87f35-df87-4697-92f9-4053cb57ec10",
   "metadata": {},
   "source": [
    "Below, we do another transformation to accomadate Q, K and V vectors. What effectively happens is that the embedding dimension gets increased by 3 times.\n",
    "This final embedding dimension will be split into three parts for Q,K and V tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c1afed8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "qkv_proj = nn.Linear(embed_dim, 3*embed_dim, bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "292c9eb6-8577-4f19-8caf-fe35050e6e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "qkv = qkv_proj(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "eab5c6e8-6d8f-4502-969b-c6d6c6d0e858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 12])\n"
     ]
    }
   ],
   "source": [
    "print(qkv.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26923a21",
   "metadata": {},
   "source": [
    "Now let's reshape qkv to accommodate the heads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "924c3f7a-1eb4-4f07-8409-ee479e4132fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "qkv = qkv.reshape(batch_size, seq_length, num_heads, 3*head_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fb4f8221-06dd-40aa-a4d3-e9d494a4cc08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 2, 6])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qkv.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377ce619-14bb-4bac-92f5-1d1e8fec7a35",
   "metadata": {},
   "source": [
    "We have two data points, each contain 4 tokens. Each token is featuerized using 6 numbers (embedding dimension). And we have two such representations (two heads)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bd01e351-9899-4ab1-903c-8ce8be41a70e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1592, -0.3586,  0.5629, -0.5026, -0.3295,  0.6718],\n",
       "         [ 0.1544, -0.4201,  0.1823, -0.1312,  0.1394, -0.2156]],\n",
       "\n",
       "        [[-0.2941,  0.1607, -0.5823,  0.5015,  0.6178, -0.5147],\n",
       "         [-0.0365,  0.5209,  0.3704,  0.3071, -0.3399,  0.4653]],\n",
       "\n",
       "        [[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# qkv for data point 1.\n",
    "qkv[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "71832ebc-c9b4-4e95-9f13-b29d13d25139",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 2, 6])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qkv[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "634cb8e2-04e7-4054-9455-9941401bd1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# permute the tensor so that we have [batch_size, num_heads, seq_length, 3*head_dim]\n",
    "qkv = qkv.permute(0, 2, 1, 3) # [batch_size, num_heads, seq_length, 3*head_dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "530689ed-2cf4-4d7c-b3d4-516ae41b964c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 4, 6])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qkv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2d375c84-f483-40d3-a1e3-f46cda94b161",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1592, -0.3586,  0.5629, -0.5026, -0.3295,  0.6718],\n",
       "         [-0.2941,  0.1607, -0.5823,  0.5015,  0.6178, -0.5147],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[ 0.1544, -0.4201,  0.1823, -0.1312,  0.1394, -0.2156],\n",
       "         [-0.0365,  0.5209,  0.3704,  0.3071, -0.3399,  0.4653],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qkv[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8590384",
   "metadata": {},
   "source": [
    "Seperate q, k and v."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fcfc2f49-9347-4261-ad01-b8eaf1d53f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "q, k, v = qkv.chunk(3, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4482285b-b7dc-4ad4-a998-3d814a6231b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 4, 2])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "29163309-ff82-4c2e-837a-7fa331c09c1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.1592, -0.3586],\n",
       "          [-0.2941,  0.1607],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000]],\n",
       "\n",
       "         [[ 0.1544, -0.4201],\n",
       "          [-0.0365,  0.5209],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000]]],\n",
       "\n",
       "\n",
       "        [[[ 0.1592, -0.3586],\n",
       "          [-0.2941,  0.1607],\n",
       "          [ 0.1441, -0.9701],\n",
       "          [ 0.2768,  0.3215]],\n",
       "\n",
       "         [[ 0.1544, -0.4201],\n",
       "          [-0.0365,  0.5209],\n",
       "          [-0.4737, -0.3240],\n",
       "          [-0.0634,  0.1860]]]], grad_fn=<SplitBackward0>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2ff62f",
   "metadata": {},
   "source": [
    "Obtain the attention logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b32b99ae-0ea1-48f4-9912-89728eb3d162",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_k = q.size()[-1]\n",
    "attn_logits = torch.matmul(q, k.transpose(-2, -1))\n",
    "attn_logits = attn_logits / math.sqrt(d_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b29bb940-d427-4dfc-a412-bb18eb998b52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 4, 4])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bfd0e55",
   "metadata": {},
   "source": [
    "Take care of the masked tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "939474fd-4bc1-41f9-a563-be1718fbc0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = s.eq(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5a461171-0a24-4227-8c8a-273785819e12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False,  True,  True],\n",
       "        [False, False, False, False]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0e9b5f97-15f7-4f03-b86c-dfd3248c4350",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = mask.unsqueeze(1).unsqueeze(2).to(torch.bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4c77a546-fea0-416d-bbd9-6232034c1b1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[False, False,  True,  True]]],\n",
       "\n",
       "\n",
       "        [[[False, False, False, False]]]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24868db7",
   "metadata": {},
   "source": [
    "Apply the mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "5dfc35e0-a376-4025-9b27-2a26eb53c2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_logits=attn_logits.masked_fill(mask, float('-inf') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c6629d5b-5844-45c6-a374-57ea23f3836d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 1.9084e-01, -1.9272e-01,  0.0000e+00,  0.0000e+00],\n",
       "          [-1.7417e-01,  1.7806e-01,  0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "         [[ 5.8889e-02, -5.0800e-02,  0.0000e+00,  0.0000e+00],\n",
       "          [-5.3038e-02,  1.0355e-01,  0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]],\n",
       "\n",
       "\n",
       "        [[[ 1.9084e-01, -1.9272e-01,  2.7438e-01, -1.7331e-01],\n",
       "          [-1.7417e-01,  1.7806e-01, -2.2569e-01,  1.0025e-01],\n",
       "          [ 4.0216e-01, -4.0335e-01,  6.1004e-01, -4.3979e-01],\n",
       "          [-4.0753e-03,  2.6458e-05, -5.2432e-02,  1.1283e-01]],\n",
       "\n",
       "         [[ 5.8889e-02, -5.0800e-02, -1.3966e-01,  2.9043e-02],\n",
       "          [-5.3038e-02,  1.0355e-01,  2.7758e-01, -6.9283e-02],\n",
       "          [-3.1021e-02, -1.9444e-01, -5.0737e-01,  1.4975e-01],\n",
       "          [-2.5426e-02,  2.3800e-02,  6.5203e-02, -1.3932e-02]]]],\n",
       "       grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e4103969-c2b4-4732-b8db-66a0f5fe673a",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention = F.softmax(attn_logits, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d1aaa370-dabd-4482-aab2-2f47c94365e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.2999, 0.2044, 0.2478, 0.2478],\n",
       "          [0.2082, 0.2961, 0.2478, 0.2478],\n",
       "          [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "          [0.2500, 0.2500, 0.2500, 0.2500]],\n",
       "\n",
       "         [[0.2644, 0.2370, 0.2493, 0.2493],\n",
       "          [0.2337, 0.2733, 0.2465, 0.2465],\n",
       "          [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "          [0.2500, 0.2500, 0.2500, 0.2500]]],\n",
       "\n",
       "\n",
       "        [[[0.2887, 0.1968, 0.3139, 0.2006],\n",
       "          [0.2133, 0.3034, 0.2026, 0.2807],\n",
       "          [0.3217, 0.1437, 0.3960, 0.1386],\n",
       "          [0.2450, 0.2461, 0.2335, 0.2754]],\n",
       "\n",
       "         [[0.2713, 0.2431, 0.2224, 0.2633],\n",
       "          [0.2200, 0.2573, 0.3062, 0.2165],\n",
       "          [0.2726, 0.2315, 0.1693, 0.3266],\n",
       "          [0.2406, 0.2527, 0.2634, 0.2433]]]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e15dd867-64bd-4ff8-b176-6582ea79ae67",
   "metadata": {},
   "outputs": [],
   "source": [
    "values = torch.matmul(attention, v)\n",
    "values = values.permute(0, 2, 1, 3) # [Batch, SeqLen, Head, Dims]\n",
    "values = values.reshape(batch_size, seq_length, embed_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7bd4b674-1ea7-4309-862b-1f5d756ec576",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0274,  0.0963, -0.0437,  0.0533],\n",
       "         [ 0.1143, -0.0125, -0.0603,  0.0768],\n",
       "         [ 0.0721,  0.0393, -0.0501,  0.0624],\n",
       "         [ 0.0721,  0.0393, -0.0501,  0.0624]],\n",
       "\n",
       "        [[ 0.1522,  0.3589,  0.1203,  0.2496],\n",
       "         [ 0.2690,  0.1288,  0.1019,  0.1842],\n",
       "         [ 0.0879,  0.5008,  0.1444,  0.3327],\n",
       "         [ 0.2238,  0.2107,  0.1110,  0.2233]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
